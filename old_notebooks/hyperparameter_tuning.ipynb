{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\alvin\\anaconda3\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alvin\\anaconda3\\lib\\site-packages (67.6.0)\n",
      "Processing c:\\users\\alvin\\desktop\\ir_pro_jolene\\pycld2-0.41-cp39-cp39-win_amd64.whl\n",
      "Installing collected packages: pycld2\n",
      "Successfully installed pycld2-0.41\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install --upgrade setuptools\n",
    "!pip install pycld2-0.41-cp39-cp39-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "F0bdYybyzm2i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ltP548wxzxcZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\alvin\\anaconda3\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from nlpaug) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from nlpaug) (2.28.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from nlpaug) (1.4.4)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from nlpaug) (4.6.4)\n",
      "Requirement already satisfied: six in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.11.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.64.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2022.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YoMrTyDyzxfA"
   },
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9w00zltDzxhd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textattack\n",
      "  Using cached textattack-0.3.8-py3-none-any.whl (418 kB)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (1.9.1)\n",
      "Collecting lru-dict\n",
      "  Using cached lru_dict-1.1.8-cp39-cp39-win_amd64.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (1.21.5)\n",
      "Collecting torch!=1.8,>=1.7.0\n",
      "  Using cached torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (1.4.4)\n",
      "Collecting num2words\n",
      "  Using cached num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "Collecting pinyin==0.4.0\n",
      "  Using cached pinyin-0.4.0-py3-none-any.whl\n",
      "Collecting editdistance\n",
      "  Using cached editdistance-0.6.2-cp39-cp39-win_amd64.whl (22 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (3.6.0)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (4.64.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (1.7.1)\n",
      "Collecting lemminflect\n",
      "  Using cached lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "Collecting bert-score>=0.3.5\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (3.7)\n",
      "Collecting transformers>=4.21.0\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting OpenHowNet\n",
      "  Using cached OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pycld2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (0.41)\n",
      "Collecting flair\n",
      "  Using cached flair-0.12-py3-none-any.whl (374 kB)\n",
      "Requirement already satisfied: word2number in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from textattack) (1.1)\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting terminaltables\n",
      "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Collecting language-tool-python\n",
      "  Using cached language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
      "Collecting datasets==2.4.0\n",
      "  Using cached datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (2022.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (2.28.1)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (0.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (21.3)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from bert-score>=0.3.5->textattack) (3.5.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from click<8.1.0->textattack) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->textattack) (2022.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.7.0->textattack) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from transformers>=4.21.0->textattack) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from transformers>=4.21.0->textattack) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from transformers>=4.21.0->textattack) (0.13.2)\n",
      "Requirement already satisfied: tabulate in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (0.8.10)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.0\n",
      "  Using cached transformer_smaller_training_vocab-0.2.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (4.9.1)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: gensim>=3.8.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (4.1.2)\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (1.0.2)\n",
      "Requirement already satisfied: boto3 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (1.24.28)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting flair\n",
      "  Using cached flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (0.1.95)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Using cached konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from flair->textattack) (2.1.0)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair->textattack) (4.11.1)\n",
      "Requirement already satisfied: six in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair->textattack) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from nltk->textattack) (1.1.0)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting anytree\n",
      "  Using cached anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from OpenHowNet->textattack) (67.6.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from gensim>=3.8.0->flair->textattack) (5.2.1)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair->textattack) (2.8.4)\n",
      "Requirement already satisfied: future in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair->textattack) (0.18.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair->textattack) (2.0.0)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Using cached importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Using cached overrides-3.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair->textattack) (2.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.4.0->textattack) (21.4.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from ftfy->flair->textattack) (0.2.5)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "Collecting dill<0.3.6\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair->textattack) (3.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\alvin\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack) (2.3.1)\n",
      "Installing collected packages: py4j, pptree, pinyin, overrides, mpld3, lru-dict, jieba, janome, docopt, xxhash, torch, terminaltables, segtok, pyarrow, num2words, multidict, more-itertools, lemminflect, langdetect, importlib-metadata, ftfy, frozenlist, editdistance, dill, deprecated, conllu, async-timeout, anytree, yarl, wikipedia-api, responses, OpenHowNet, multiprocess, language-tool-python, konoha, hyperopt, huggingface-hub, aiosignal, transformers, gdown, bpemb, aiohttp, flair, bert-score, datasets, textattack\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 4.6.4\n",
      "    Uninstalling gdown-4.6.4:\n",
      "      Successfully uninstalled gdown-4.6.4\n",
      "Successfully installed OpenHowNet-2.0 aiohttp-3.8.4 aiosignal-1.3.1 anytree-2.8.0 async-timeout-4.0.2 bert-score-0.3.13 bpemb-0.3.4 conllu-4.5.2 datasets-2.4.0 deprecated-1.2.13 dill-0.3.5.1 docopt-0.6.2 editdistance-0.6.2 flair-0.11.3 frozenlist-1.3.3 ftfy-6.1.1 gdown-4.4.0 huggingface-hub-0.13.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 jieba-0.42.1 konoha-4.6.5 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.1.8 more-itertools-9.1.0 mpld3-0.3 multidict-6.0.4 multiprocess-0.70.13 num2words-0.5.12 overrides-3.1.0 pinyin-0.4.0 pptree-3.1 py4j-0.10.9.7 pyarrow-11.0.0 responses-0.18.0 segtok-1.5.11 terminaltables-3.1.10 textattack-0.3.8 torch-1.13.1 transformers-4.26.1 wikipedia-api-0.5.8 xxhash-3.2.0 yarl-1.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sphinx 5.0.2 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install textattack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHlYneeSzxjt",
    "outputId": "f8c161ce-bae9-49c5-805a-0d0f721dae8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Updating TextAttack package dependencies.\n",
      "textattack: Downloading NLTK required packages.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EmbeddingAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVw8al93z32j",
    "outputId": "5ffb21eb-47b5-454d-dbe0-00a7cb710a77"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "T7k-lcJgW0io"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Likes</th>\n",
       "      <th>NFT</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob</th>\n",
       "      <th>Polarity (Reviewer 1)</th>\n",
       "      <th>Polarity (Reviewer 2)\\r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#27894...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üê≥1 Mutant Ape Yacht Club bought for Œû15.377\\n\\...</td>\n",
       "      <td>mutant ape yacht club bought floor h chg floor...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#77 so...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#23452...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>Mutant Ape Yacht Club #27908 sold for 17 ETH (...</td>\n",
       "      <td>mutant ape yacht club sold eth nft collection ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Datetime  Quarter  Likes                    NFT  \\\n",
       "0  2022-06-27       22      2  Mutant Ape Yacht Club   \n",
       "1  2023-02-11       31      2  Mutant Ape Yacht Club   \n",
       "2  2022-12-30       24      0  Mutant Ape Yacht Club   \n",
       "3  2022-12-29       24      0  Mutant Ape Yacht Club   \n",
       "4  2022-12-29       24      1  Mutant Ape Yacht Club   \n",
       "\n",
       "                                                Text  \\\n",
       "0  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#27894...   \n",
       "1  üê≥1 Mutant Ape Yacht Club bought for Œû15.377\\n\\...   \n",
       "2  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#77 so...   \n",
       "3  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#23452...   \n",
       "4  Mutant Ape Yacht Club #27908 sold for 17 ETH (...   \n",
       "\n",
       "                                          Clean Text vader textblob  \\\n",
       "0          mutant ape yacht club mayc nft sold eth k   neu      neu   \n",
       "1  mutant ape yacht club bought floor h chg floor...   neu      neu   \n",
       "2          mutant ape yacht club mayc nft sold eth k   neu      neu   \n",
       "3          mutant ape yacht club mayc nft sold eth k   neu      neu   \n",
       "4  mutant ape yacht club sold eth nft collection ...   neu      neu   \n",
       "\n",
       "  Polarity (Reviewer 1) Polarity (Reviewer 2)\\r  \n",
       "0                   pos                   pos\\r  \n",
       "1                   pos                   pos\\r  \n",
       "2                   pos                   pos\\r  \n",
       "3                   pos                   pos\\r  \n",
       "4                   pos                   pos\\r  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('label_dataset.csv', lineterminator='\\n')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "X7isxrdkW0io"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Likes</th>\n",
       "      <th>NFT</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob</th>\n",
       "      <th>label</th>\n",
       "      <th>Polarity (Reviewer 2)\\r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#27894...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üê≥1 Mutant Ape Yacht Club bought for Œû15.377\\n\\...</td>\n",
       "      <td>mutant ape yacht club bought floor h chg floor...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#77 so...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#23452...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>Mutant Ape Yacht Club #27908 sold for 17 ETH (...</td>\n",
       "      <td>mutant ape yacht club sold eth nft collection ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Datetime  Quarter  Likes                    NFT  \\\n",
       "0  2022-06-27       22      2  Mutant Ape Yacht Club   \n",
       "1  2023-02-11       31      2  Mutant Ape Yacht Club   \n",
       "2  2022-12-30       24      0  Mutant Ape Yacht Club   \n",
       "3  2022-12-29       24      0  Mutant Ape Yacht Club   \n",
       "4  2022-12-29       24      1  Mutant Ape Yacht Club   \n",
       "\n",
       "                                                Text  \\\n",
       "0  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#27894...   \n",
       "1  üê≥1 Mutant Ape Yacht Club bought for Œû15.377\\n\\...   \n",
       "2  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#77 so...   \n",
       "3  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#23452...   \n",
       "4  Mutant Ape Yacht Club #27908 sold for 17 ETH (...   \n",
       "\n",
       "                                          Clean Text vader textblob label  \\\n",
       "0          mutant ape yacht club mayc nft sold eth k   neu      neu   pos   \n",
       "1  mutant ape yacht club bought floor h chg floor...   neu      neu   pos   \n",
       "2          mutant ape yacht club mayc nft sold eth k   neu      neu   pos   \n",
       "3          mutant ape yacht club mayc nft sold eth k   neu      neu   pos   \n",
       "4  mutant ape yacht club sold eth nft collection ...   neu      neu   pos   \n",
       "\n",
       "  Polarity (Reviewer 2)\\r  \n",
       "0                   pos\\r  \n",
       "1                   pos\\r  \n",
       "2                   pos\\r  \n",
       "3                   pos\\r  \n",
       "4                   pos\\r  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Polarity (Reviewer 1)':'label'}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ILeWpfPmQGvi"
   },
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace('neg',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9shnCakaQG27"
   },
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace('pos',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Oh2SvmYPQG9Z"
   },
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace('neu',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "g_TyvzSpWBgI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Likes</th>\n",
       "      <th>NFT</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob</th>\n",
       "      <th>label</th>\n",
       "      <th>Polarity (Reviewer 2)\\r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#27894...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üê≥1 Mutant Ape Yacht Club bought for Œû15.377\\n\\...</td>\n",
       "      <td>mutant ape yacht club bought floor h chg floor...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#77 so...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#23452...</td>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>Mutant Ape Yacht Club #27908 sold for 17 ETH (...</td>\n",
       "      <td>mutant ape yacht club sold eth nft collection ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pos\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Datetime  Quarter  Likes                    NFT  \\\n",
       "0  2022-06-27       22      2  Mutant Ape Yacht Club   \n",
       "1  2023-02-11       31      2  Mutant Ape Yacht Club   \n",
       "2  2022-12-30       24      0  Mutant Ape Yacht Club   \n",
       "3  2022-12-29       24      0  Mutant Ape Yacht Club   \n",
       "4  2022-12-29       24      1  Mutant Ape Yacht Club   \n",
       "\n",
       "                                                Text  \\\n",
       "0  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#27894...   \n",
       "1  üê≥1 Mutant Ape Yacht Club bought for Œû15.377\\n\\...   \n",
       "2  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#77 so...   \n",
       "3  üß™ Mutant Ape Yacht Club | #MAYC #NFT\\n\\n#23452...   \n",
       "4  Mutant Ape Yacht Club #27908 sold for 17 ETH (...   \n",
       "\n",
       "                                          Clean Text vader textblob  label  \\\n",
       "0          mutant ape yacht club mayc nft sold eth k   neu      neu    1.0   \n",
       "1  mutant ape yacht club bought floor h chg floor...   neu      neu    1.0   \n",
       "2          mutant ape yacht club mayc nft sold eth k   neu      neu    1.0   \n",
       "3          mutant ape yacht club mayc nft sold eth k   neu      neu    1.0   \n",
       "4  mutant ape yacht club sold eth nft collection ...   neu      neu    1.0   \n",
       "\n",
       "  Polarity (Reviewer 2)\\r  \n",
       "0                   pos\\r  \n",
       "1                   pos\\r  \n",
       "2                   pos\\r  \n",
       "3                   pos\\r  \n",
       "4                   pos\\r  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ifkIE9d4W0io"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1053\n",
       "2.0     453\n",
       "0.0      95\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fItnkpZwW0io"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mutant ape yacht club bought floor h chg floor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mutant ape yacht club mayc nft sold eth k</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mutant ape yacht club sold eth nft collection ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Clean Text  label\n",
       "0          mutant ape yacht club mayc nft sold eth k    1.0\n",
       "1  mutant ape yacht club bought floor h chg floor...    1.0\n",
       "2          mutant ape yacht club mayc nft sold eth k    1.0\n",
       "3          mutant ape yacht club mayc nft sold eth k    1.0\n",
       "4  mutant ape yacht club sold eth nft collection ...    1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Clean Text','label']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FP08YKs7W0io"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4KXT-9prW0ip"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "rvnSO_pBW0ip"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "R7z3Bfa1W0ip"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Clean Text'], df['label'], test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Lvp3lcV2W0ip"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    848\n",
       "2.0    353\n",
       "0.0     79\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "yIZXBbOlW0ip"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 1,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BAnK1h3pW0ip"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Ojs2yqvcCMvL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 481M/481M [01:16<00:00, 6.25MB/s]   \n",
      "textattack: Unzipping file C:\\Users\\Alvin\\.cache\\textattack\\tmpkya2pm6n.zip to C:\\Users\\Alvin/.cache/textattack\\word_embeddings/paragramcf.\n",
      "textattack: Successfully saved word_embeddings/paragramcf to cache.\n"
     ]
    }
   ],
   "source": [
    "embed_aug = EmbeddingAugmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "vcKNRe10CMvM"
   },
   "outputs": [],
   "source": [
    "augmented_sentences=[]\n",
    "augmented_sentences_labels=[]\n",
    "for i in X_train.index:\n",
    "  if y_train[i]==0:\n",
    "    temps3=embed_aug.augment(X_train[i])\n",
    "    for sent in temps3:\n",
    "      augmented_sentences.append(sent)\n",
    "      augmented_sentences_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "2B-ObnOECMvL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1359,)\n",
      "(1359,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_2912\\31741503.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(pd.Series(augmented_sentences),ignore_index=True)\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_2912\\31741503.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train=y_train.append(pd.Series(augmented_sentences_labels),ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.append(pd.Series(augmented_sentences),ignore_index=True)\n",
    "y_train=y_train.append(pd.Series(augmented_sentences_labels),ignore_index=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7zQrI1iQCMvL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    848\n",
       "2.0    353\n",
       "0.0    158\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "TM5aXTmmCMvL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Alvin\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet',aug_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "fngh7ZUaCMvL"
   },
   "outputs": [],
   "source": [
    "augmented_sentences=[]\n",
    "augmented_sentences_labels=[]\n",
    "for i in X_train.index:\n",
    "  if y_train[i]==0:\n",
    "    temps1=aug.augment(X_train[i],n=3)\n",
    "    for sent in temps1:\n",
    "      augmented_sentences.append(sent)\n",
    "      augmented_sentences_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "r906bi1RCMvM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1833,)\n",
      "(1833,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_2912\\31741503.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(pd.Series(augmented_sentences),ignore_index=True)\n",
      "C:\\Users\\Alvin\\AppData\\Local\\Temp\\ipykernel_2912\\31741503.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train=y_train.append(pd.Series(augmented_sentences_labels),ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.append(pd.Series(augmented_sentences),ignore_index=True)\n",
    "y_train=y_train.append(pd.Series(augmented_sentences_labels),ignore_index=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "guRz2H6LCMvM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    848\n",
       "0.0    632\n",
       "2.0    353\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "NI9LGatoCMvM"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "F0JV32rbIrU7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': None,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGB_classifier = GradientBoostingClassifier()\n",
    "XGB_classifier = XGBClassifier()\n",
    "\n",
    "XGB_classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "04sO1ruGCMvM"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "#XGB_classifier = GradientBoostingClassifier().fit(X_train_vectorized,y_train)\n",
    "XGB_classifier = XGBClassifier(seed=0).fit(X_train_vectorized,y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "time_linear_train = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22999835014343262"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "z1Lmk6Z_PgaK"
   },
   "outputs": [],
   "source": [
    "y_pred=XGB_classifier.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Xk8S6s1-CMvM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.229998s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time: %fs\" % (time_linear_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "RywKs3EfCMvM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822429906542056\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "K6QrPWtTCMvN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.56      0.58        16\n",
      "         1.0       0.86      0.91      0.88       205\n",
      "         2.0       0.78      0.69      0.73       100\n",
      "\n",
      "    accuracy                           0.82       321\n",
      "   macro avg       0.74      0.72      0.73       321\n",
      "weighted avg       0.82      0.82      0.82       321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "5CtRqi03Nnf5"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "F0LMmWwYz37T"
   },
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "param_grid = { \n",
    "    \"learning_rate\": [0.1, 0.01],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"n_estimators\": [50, 200, 300, 400, 500],\n",
    "    \"min_child_weight\": [3, 5, 7, 9],\n",
    "    \"subsample\": [0.3, 0.5, 0.8],\n",
    "    \"colsample_bytree\": [0.3, 0.5 , 0.8]\n",
    "    }\n",
    "# Set up score\n",
    "scoring = ['f1']\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "AFeP9mNDz39w",
    "outputId": "f79df816-4bc5-454d-e784-a2088211ac65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is nan\n",
      "The best hyperparameters are {'colsample_bytree': 0.3, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Define grid search\n",
    "grid_search = GridSearchCV(estimator=XGB_classifier, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring=scoring, \n",
    "                           refit='f1', \n",
    "                           n_jobs=-1, \n",
    "                           cv=kfold, \n",
    "                           verbose=0)\n",
    "# Fit grid search\n",
    "grid_result = grid_search.fit(X_train_vectorized,y_train)\n",
    "# Print grid search summary\n",
    "grid_result\n",
    "# Print the best score and the corresponding hyperparameters\n",
    "print(f'The best score is {grid_result.best_score_:.4f}')\n",
    "#print('The best score standard deviation is', round(grid_result.cv_results_['std_test_recall'][grid_result.best_index_], 4))\n",
    "print(f'The best hyperparameters are {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "DjUQQ6Ahz3_t"
   },
   "outputs": [],
   "source": [
    "# Make prediction using the best model\n",
    "grid_predict = grid_search.predict(X_test_vectorized)\n",
    "# Get predicted probabilities\n",
    "#grid_predict_prob = grid_search.predict_proba(X_test_vectorized)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "y4AWNFH5z4CP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822429906542056\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "slsiMi1wzxmB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.56      0.58        16\n",
      "         1.0       0.86      0.91      0.88       205\n",
      "         2.0       0.78      0.69      0.73       100\n",
      "\n",
      "    accuracy                           0.82       321\n",
      "   macro avg       0.74      0.72      0.73       321\n",
      "weighted avg       0.82      0.82      0.82       321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu2dap4SzxoK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOb6HAbwzxqr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
